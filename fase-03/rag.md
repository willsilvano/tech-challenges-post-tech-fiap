# ğŸ›’ Assistente de Produtos RAG

Uma aplicaÃ§Ã£o de RecuperaÃ§Ã£o Aumentada por GeraÃ§Ã£o (RAG) para responder consultas sobre produtos utilizando LangChain, FAISS, Ollama e Streamlit.

## ğŸ“Œ VisÃ£o Geral

Este projeto implementa um assistente virtual para consultas sobre produtos utilizando a tÃ©cnica RAG (Retrieval-Augmented Generation). O sistema recupera informaÃ§Ãµes relevantes de uma base de dados de produtos e gera respostas naturais em portuguÃªs, mesmo quando as perguntas sÃ£o feitas em inglÃªs.

**O Sistema tambÃ©m usa um modelo llama3.2 fine tunado para responder as perguntas.**

## ğŸš€ Funcionalidades

- **Pesquisa SemÃ¢ntica**: Encontra produtos relevantes com base no significado da consulta, nÃ£o apenas em palavras-chave
- **Interface AmigÃ¡vel**: Interface de chat intuitiva criada com Streamlit
- **TransparÃªncia**: Visualize quais documentos foram utilizados para gerar as respostas
- **PersonalizaÃ§Ã£o**: Ajuste parÃ¢metros como temperatura do modelo e limites de similaridade
- **MultilÃ­ngue**: Responde em portuguÃªs mesmo para consultas em inglÃªs

## ğŸ› ï¸ Tecnologias Utilizadas

- **LangChain**: Framework para criar aplicaÃ§Ãµes baseadas em LLMs
- **FAISS**: Biblioteca para busca de similaridade eficiente em vetores de alta dimensÃ£o
- **Ollama**: Modelos de linguagem executados localmente
- **Streamlit**: Interface de usuÃ¡rio web interativa
- **Pandas**: Processamento de dados tabulares

## ğŸ“‹ PrÃ©-requisitos

- Python 3.8+
- Ollama instalado e configurado com os modelos necessÃ¡rios
- Arquivo CSV com dados de produtos no formato adequado

## âš™ï¸ InstalaÃ§Ã£o

1. Instale as dependÃªncias:
   ```bash
   uv sync
   ```

2. Certifique-se de ter o Ollama instalado e os modelos configurados:
   ```bash
   # Instale o modelo
   AtravÃ©s do GGUF gerado no Google Colab, baixe-o e salve dentro da pasta de modelos do LMStudio.

   # FaÃ§a o deploy local dos modelos
   Na tela de `Developer` do LMStudio, selecione os dmodelos para realizar o deploy local.

   # Instale o modelo de embeddings
   ollama pull nomic-embed-text
   ```

## ğŸ“Š Estrutura de Dados

O arquivo CSV de entrada deve conter pelo menos as seguintes colunas:
- `title`: TÃ­tulo do produto
- `content`: DescriÃ§Ã£o ou detalhes do produto

Exemplo:
```csv
title,content,price,category
"Smartphone XYZ","Um smartphone avanÃ§ado com cÃ¢mera de 48MP e tela AMOLED.",999.99,EletrÃ´nicos
```

## ğŸš€ Uso

1. Inicie a aplicaÃ§Ã£o:
   ```bash
   streamlit run app.py
   ```

2. No painel lateral:
   - Selecione os modelos e parÃ¢metros desejados
   - Clique em "Inicializar Assistente"
   - Clique em "Processar Dados CSV" para carregar seus dados

3. Na Ã¡rea principal, faÃ§a perguntas sobre produtos usando o campo de chat

## ğŸ”§ ConfiguraÃ§Ãµes AvanÃ§adas

### Modelos Suportados
- LLM: `llama3.2-3b-perguntas`, `llama3.2:3b`
- Embeddings: `nomic-embed-text`

### Ajustes de ParÃ¢metros
- **Temperatura**: Afeta a criatividade das respostas (0.0 - 1.0)
- **Limiar de Similaridade**: Filtra documentos com base na relevÃ¢ncia (0% - 100%)
- **NÃºmero de Documentos (k)**: Quantidade de documentos recuperados para cada consulta (1-10)

## ğŸ“ Estrutura do Projeto

```
assistente-produtos-rag/
â”œâ”€â”€ rag.py                # AplicaÃ§Ã£o Streamlit principal
â”œâ”€â”€ data/                 # DiretÃ³rio para arquivos de dados
â”‚   â””â”€â”€ data-1000.csv     # Exemplo de dados de produtos
â”œâ”€â”€ vector_store/         # DiretÃ³rio para armazenar Ã­ndices FAISS
â”œâ”€â”€ requirements.txt      # DependÃªncias do projeto
â””â”€â”€ README.md             # DocumentaÃ§Ã£o
```

## ğŸ“ Exemplo de Consultas

- "Samsung Galaxy Note II Decal Vinyl Skin Ã© uma boa compra?"
- "I want car decals"
- "Recommends me products for 3D printer"
- "HÃ¡ algum item de automÃ³veis na sua loja?"
- "Zoya Nail Polish .5 fl oz Ã© uma boa compra?"